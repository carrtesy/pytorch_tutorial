{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpIj0EhRWZscVJTpEYH6eg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dongminkim0220/pytorch_tutorial/blob/master/RL_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5OGSJtMIEzl"
      },
      "source": [
        "# RL with DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuivbdPMILb9"
      },
      "source": [
        "import gym\n",
        "import random, math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSRDw7H3KqsO"
      },
      "source": [
        "## hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fdbeBnIIdg9"
      },
      "source": [
        "EPISODES = 50\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "GAMMA = 0.8\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-2JI12xKtOm"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIgKGPTWIuF5"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
        "        self.steps_done = 0\n",
        "        self.memory = deque(maxlen = 10000)\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append(\n",
        "            (state, action, torch.FloatTensor([reward]), torch.FloatTensor([next_state]))\n",
        "        )\n",
        "\n",
        "    def act(self, state):\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * self.steps_done / EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "        if random.random() > eps_threshold:\n",
        "            return self.model(state).data.max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.LongTensor([[random.randrange(2)]])\n",
        "    \n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.cat(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "\n",
        "        current_q = self.model(states).gather(1, actions)\n",
        "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
        "        expected_q = rewards + (GAMMA * max_next_q)\n",
        "\n",
        "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4wPf4uSKlQS"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6A8tQoeKpJr"
      },
      "source": [
        "agent = DQNAgent()\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "score_history = []\n",
        "frames = []\n",
        "\n",
        "for e in range(1, EPISODES + 1):\n",
        "    \n",
        "    steps = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        state = torch.FloatTensor([state])\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        \n",
        "        if done:\n",
        "            reward = -1\n",
        "        \n",
        "        agent.memorize(state, action, reward, next_state)\n",
        "        agent.learn()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            print(f\"Episode {e} | Score {steps}\")\n",
        "            score_history.append(steps)\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnA11HYdLpVH"
      },
      "source": [
        "plt.plot(score_history)\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}